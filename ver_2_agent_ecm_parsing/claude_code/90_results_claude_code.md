# ECM Atlas Dataset Parsing - Analysis Results (Claude Code)

**Date:** 2025-10-12
**Agent:** Claude Code CLI
**Task:** Version 2 Analysis - Knowledge-First Approach
**Status:** ANALYSIS COMPLETE - READY FOR EXECUTION

---

## Executive Summary

This document presents a comprehensive analysis of the ECM Atlas dataset parsing task (Version 2), identifying critical issues, data complexities, and providing a detailed implementation roadmap.

### Key Findings

**üìä Task Scope:**
- ‚úÖ 11 datasets identified for parsing (excluding Lofaro PDF, McCabe DOCX)
- ‚ö†Ô∏è 1 dataset ambiguous (Ouni 2022 - may be literature mining, not proteomics)
- ‚úÖ Expected output: 150,000-200,000 rows, 12,000+ unique proteins
- ‚úÖ 25 success criteria (up from 21 in V1) - 100% required for PASS

**üîç Critical Gaps Identified:**
1. **Missing paper citations:** Need PMID/DOI for abundance formulas and age definitions
2. **Dataset inventory incomplete:** 4 studies lack detailed structure analysis (Angelidis, Caldeira, Dipali, Ouni)
3. **Normalization strategy undefined:** No decision on how to handle diverse abundance units
4. **Ouni 2022 ambiguity:** Conflicting info on whether this is proteomics data

**‚úÖ Strengths of Existing Work:**
- V1 code architecture is solid (config-driven OOP)
- 7 datasets already have detailed structure documentation
- Clear schema defined with new Parsing_Notes column
- Comprehensive success rubric with specific evidence requirements

**üéØ Recommended Approach:**
1. **TIER 0 FIRST:** Complete knowledge base before any coding (40-60 min)
2. **Incremental parsing:** Start with 3 easy datasets, validate, then continue
3. **Extend V1 code:** Add Parsing_Notes generation layer to proven architecture
4. **Checkpoint validation:** After every 3 datasets to catch issues early

---

## Dataset-by-Dataset Analysis

### 1. Angelidis et al. 2019 ‚úÖ

**Status:** ‚úÖ PARSED IN V1 (38,057 rows, 5,189 proteins)

**Files Available:**
- 10 Excel files (MOESM4-10)
- **Primary:** `41467_2019_8831_MOESM5_ESM.xlsx` (confirmed from V1)

**Structure:**
- Sheet: "Proteome"
- Protein IDs: `Majority protein IDs` column
- Gene symbols: `Gene names`
- Protein names: `Protein names`
- Abundance: 8 sample columns (`old_1` to `old_4`, `young_1` to `young_4`)
- Units: log2_intensity (LFQ already log2-transformed)

**Age Mapping:**
- old = 24 months (from paper Methods)
- young = 3 months (from paper Methods)

**Parsing Strategy:** ‚úÖ Already working in V1, reuse

**Gaps:**
- ‚ùå Missing exact Methods section citation (need "page 2, paragraph 3" specificity)
- ‚ùå Missing abundance formula quote
- ‚ùå Need to add Parsing_Notes generation

**Parsing_Notes Template:**
```
"Age={age}mo from col name '{col_name}' per Methods p.X; Abundance from MOESM5 sheet 'Proteome' col {col}; log2 already applied per Methods p.Y"
```

---

### 2. Ariosa-Morejon et al. 2021 ‚ö†Ô∏è

**Status:** ‚ö†Ô∏è ANALYZED BUT NOT PARSED

**Files Available:**
- `elife-66635-fig2-data1-v1.xlsx` (1.05 MB)

**Structure:**
- **Multi-sheet:** Plasma (173 rows), Cartilage (634), Bone (712), Skin (352)
- Protein IDs: `Majority protein IDs`
- Gene symbols: `Gene names`
- Protein names: `Protein names` (note: may have leading space - needs strip())
- Abundance: iBAQ values and H/L ratios
  - `iBAQ L A1-4` (Light isotope = young)
  - `iBAQ H A1-4` (Heavy isotope = old)
  - `Ratio H/L A1-4`, `Ratio H/L B1-4`

**Age Mapping:**
- Heavy (H) = old
- Light (L) = young
- Group A vs Group B (biological replicates)

**Complexity:**
- Multi-sheet (4 tissues)
- Ratio-based abundance
- ~21-30 columns per tissue

**Parsing Strategy:**
1. Loop through 4 tissue sheets
2. Extract iBAQ values (not ratios - use raw abundance)
3. Map H samples to "old", L samples to "young"
4. Tissue from sheet name

**Gaps:**
- ‚ùå Need exact age values (months/years) from paper
- ‚ùå Need abundance formula quote
- ‚ùå Unclear if iBAQ or Ratio should be used as Abundance

**Estimated Rows:** ~1,900 (173+634+712+352) √ó samples = ~5,000-8,000

---

### 3. Caldeira et al. 2017 ‚úÖ

**Status:** ‚úÖ PARSED IN V1 (1,078 rows, 77 proteins)

**Files Available:**
- `41598_2017_11960_MOESM2_ESM.xls` (old .xls format)

**Structure:**
- 3 age groups: Foetus, Young, Old
- Tissue: Cartilage
- Species: Bos taurus (cow)
- Abundance_Unit: normalized_ratio
- Method: LC-MS/MS

**Parsing Strategy:** ‚úÖ Already working in V1, reuse

**Gaps:**
- ‚ùå Missing exact age definitions (Foetus = ?, Young = ?, Old = ?)
- ‚ùå Missing abundance normalization formula
- ‚ùå Need to add Parsing_Notes

**Estimated Rows:** 1,078 (confirmed from V1)

---

### 4. Chmelova et al. 2023 üî¥

**Status:** üî¥ COMPLEX - TRANSPOSED FORMAT

**Files Available:**
- `Data Sheet 1.XLSX` (758 KB)

**Structure:**
- **CRITICAL:** TRANSPOSED matrix (samples as rows, genes as columns)
- Dimensions: 17 rows √ó 3,830 columns
- Row 0 (samples): `X3m_ctrl_A`, `X18m_ctrl_B`, `X3m_MCAO_3d_C`, etc.
- Columns (genes): `Ighv1.31`, `Igha`, `Ptprf`, etc.
- Values: Log2-transformed protein abundance

**Sample Types:**
- `X3m_ctrl_*` - 3 month controls ‚úÖ USE
- `X18m_ctrl_*` - 18 month controls ‚úÖ USE
- `X3m_MCAO_*` - 3 month stroke ‚ùå EXCLUDE
- `X18m_MCAO_*` - 18 month stroke ‚ùå EXCLUDE

**Age Mapping:**
- 3m (3 months)
- 18m (18 months)

**Parsing Strategy:**
1. **Transpose** DataFrame (critical!)
2. Filter columns to only `ctrl` samples (exclude MCAO)
3. Gene symbols become Protein_ID (no UniProt IDs available)
4. Each gene √ó sample = 1 row

**Complexity:**
- Matrix needs transposition
- Gene symbols only (no UniProt mapping)
- Sample filtering required
- Tissue: Brain (need to confirm from paper)
- Species: Mus musculus (need to confirm)

**Gaps:**
- ‚ùå Missing tissue confirmation (likely brain from stroke model)
- ‚ùå Missing species confirmation (likely mouse)
- ‚ùå Missing abundance unit details (log2 of what?)
- ‚ùå No Protein_Name available (only gene symbols)

**Estimated Rows:** 3,830 genes √ó ~5 control samples (3√ó3m + 2√ó18m) = ~19,000 rows

---

### 5. Dipali et al. 2023 ‚úÖ

**Status:** ‚úÖ PARSED IN V1 (8,168 rows, 4,084 proteins)

**Files Available:**
- `Candidates_210823_SD7_Native_Ovary_v7_directDIA_v3.xlsx`
- Multiple other files (decellularized, overlap)

**Structure:**
- 2 age groups: Old, Young
- Tissue: Ovary
- Species: Mus musculus
- Method: DIA (Data-Independent Acquisition)
- Abundance_Unit: LFQ

**Parsing Strategy:** ‚úÖ Already working in V1, reuse

**Gaps:**
- ‚ùå Missing exact age definitions (Old = ? months, Young = ? months)
- ‚ùå Missing DirectDIA normalization details
- ‚ùå Need to add Parsing_Notes

**Estimated Rows:** 8,168 (confirmed from V1)

---

### 6. Li et al. 2021 | dermis ‚ö†Ô∏è

**Status:** ‚ö†Ô∏è ANALYZED BUT NOT PARSED

**Files Available:**
- `Table 1.xlsx`, `Table 2.xlsx`, `Table 3.xlsx`, `Table 4.xlsx`
- **Primary:** `Table 2.xlsx` (proteomics data)

**Structure:**
- **Skip rows:** 3 (title and description rows embedded)
- Dimensions: 264 rows √ó 22 columns
- Protein IDs: `Protein ID` (column 0) - after skiprows
- Gene symbols: `Gene symbol` (column 1) - after skiprows
- ECM classification: `ECM components`, subcategory (columns 2-3)
- Abundance: Columns 4-18 (15 samples)
  - `Toddler-Sample1-2` (2 samples)
  - `Teenager-Sample1-3` (3 samples)
  - `Adult-Sample1-3` (3 samples)
  - `Elderly-Sample1-5` (5 samples)
- Values: log2-normalized FOT (Fraction of Total)

**Age Mapping:**
- Toddler = ?
- Teenager = ?
- Adult = ?
- Elderly = ?
(Need exact ages from paper)

**Tissue:** Dermis (skin)
**Species:** Homo sapiens

**Parsing Strategy:**
1. `pd.read_excel(skiprows=3)` - critical!
2. First row becomes headers
3. Rename columns 0-1 if needed
4. Extract age group from column name prefix
5. Parse sample number from suffix

**Complexity:**
- Header row embedded in data (skiprows=3)
- 4 age groups with uneven samples
- ECM classification available (bonus metadata)

**Gaps:**
- ‚ùå Missing exact age definitions for 4 groups
- ‚ùå Missing FOT normalization details
- ‚ùå Need sheet name (likely "Table S2")

**Estimated Rows:** 264 proteins √ó 15 samples = ~4,000 rows

---

### 7. Li et al. 2021 | pancreas ‚ö†Ô∏è

**Status:** ‚ö†Ô∏è ANALYZED BUT NOT PARSED

**Files Available:**
- Multiple MOESM files (4-13)
- **Primary:** `41467_2021_21261_MOESM6_ESM.xlsx`

**Structure:**
- **Skip rows:** 2 (title rows)
- Sheet: "Data 3"
- Dimensions: 2,064 rows √ó 53 columns
- Protein IDs: `Accession` (column 0)
- Protein names: `Description` (column 1)
- Abundance: Columns 2-25 (24 samples)
  - `F_7, F_8, F_9, F_10, F_11, F_12` (6 fetal)
  - `J_8, J_22, J_46, J_59, J_67` (5 juvenile - weeks)
  - `Y_12, Y_31, Y_51, Y_57, Y_61, Y_64` (6 young - years)
  - `O_14, O_20, O_48, O_52, O_54, O_68` (6 old - years)

**Age Mapping:**
- **F** = Fetal (number = gestational week?)
- **J** = Juvenile (number = weeks old)
- **Y** = Young adult (number = years old)
- **O** = Old (number = years old)

**Tissue:** Pancreas
**Species:** Homo sapiens

**Parsing Strategy:**
1. `pd.read_excel(skiprows=2, sheet_name="Data 3")`
2. Parse column prefix (F/J/Y/O) ‚Üí age group
3. Parse number after underscore ‚Üí age value
4. Set Age_Unit based on group (weeks for F/J, years for Y/O)

**Complexity:**
- 4 age groups with different units (weeks vs years)
- Age value encoded in column name
- Fetal age may be gestational weeks (need confirmation)

**Gaps:**
- ‚ùå Need exact age group definitions from paper
- ‚ùå Confirm fetal age interpretation (gestational weeks?)
- ‚ùå Missing abundance unit details

**Estimated Rows:** 2,064 proteins √ó 24 samples = ~50,000 rows

---

### 8. Ouni et al. 2022 üî¥

**Status:** üî¥ AMBIGUOUS - MAY NOT BE PROTEOMICS

**Files Available:**
- `Supp Table 1.xlsx`, `Supp Table 2.xlsx`, `Supp Table 3.xlsx`, `Supp Table 4.xlsx`

**Conflicting Information:**
- **PARSER_CONFIG_SUMMARY.md:** "SKIP - Literature mining data, not proteomics"
- **Task V2:** Lists as dataset #8 to parse (suggests ~10,000 rows)

**Investigation Needed:**
- Check Table 1-4 structure
- Look for protein abundance columns
- Determine if this is proteomic data or pathway/literature analysis

**Possible Outcomes:**
- ‚úÖ If proteomics ‚Üí Parse (11 datasets total)
- ‚ùå If literature mining ‚Üí Exclude (10 datasets total)

**Impact on Success Criteria:**
- 11 datasets: Target ‚â•150,000 rows
- 10 datasets: Target ‚â•140,000 rows (still acceptable)

**Action Required:** Investigate files FIRST before committing to parse

---

### 9. Randles et al. 2021 ‚ö†Ô∏è

**Status:** ‚ö†Ô∏è ANALYZED BUT NOT PARSED

**Files Available:**
- `ASN.2020101442-File027.xlsx` (1.35 MB) ‚úÖ EXCEL
- `index.html` (alternative HTML format)

**Structure:**
- **Skip rows:** 1 (merged header row)
- Sheet: "Human data matrix fraction"
- Dimensions: 2,610 rows √ó 31 columns
- Protein IDs: `Accession` (column 1)
- Gene symbols: `Gene name` (column 0)
- Protein names: `Description` (column 6)
- Abundance: Columns 7-18 (12 samples)
  - Glomerular: `G15, G29, G37, G61, G67, G69`
  - Tubular: `T15, T29, T37, T61, T67, T69`
- Units: Hi-N normalized abundance

**Age Mapping:**
- Number in column name = patient age in years
- Ages: 15, 29, 37, 61, 67, 69 years

**Tissue:** Kidney (2 compartments)
- G = Glomerular
- T = Tubular

**Species:** Homo sapiens

**Parsing Strategy:**
1. `pd.read_excel(skiprows=1, sheet_name="Human data matrix fraction")`
2. Extract age from column name (number after G/T)
3. Extract tissue from column prefix (G=Glomerular, T=Tubular)
4. Each protein √ó sample = 1 row

**Complexity:**
- Dual tissue type (Glomerular vs Tubular)
- Age directly in column names (easiest!)
- Skip row required

**Gaps:**
- ‚ùå Missing Hi-N normalization details (what is Hi-N?)
- ‚ùå Need Method confirmation (LC-MS/MS?)

**Estimated Rows:** 2,610 proteins √ó 12 samples = ~31,000 rows

---

### 10. Tam et al. 2020 üî¥

**Status:** üî¥ COMPLEX - HIERARCHICAL COLUMNS

**Files Available:**
- `elife-64940-supp1-v3.xlsx` (1.42 MB)

**Structure:**
- **Skip rows:** 1 (metadata labels like "T: Majority protein IDs")
- Sheet: "Raw data"
- Dimensions: 3,157 rows √ó 80 columns
- Protein IDs: `T: Majority protein IDs` ‚Üí `Majority protein IDs` (remove "T: " prefix)
- Protein names: `T: Protein names` ‚Üí `Protein names`
- Gene symbols: `T: Gene names` ‚Üí `Gene names`
- Abundance: Columns 3-68 (66 LFQ columns)

**Column Pattern:** `LFQ intensity L3/4 old L OAF`
- **Disc level:** L3/4, L4/5, L5/S1 (3 options)
- **Age:** "old" vs "Young" (2 options)
- **Region:** L (left), A (anterior), P (posterior), R (right) (4 options)
- **Tissue:** OAF (outer annulus), IAF (inner annulus), IAF/NP, NP (nucleus) (4 options)

**Combinations:** 3 √ó 2 √ó 4 √ó 4 = 96 possible (but only 66 present)

**Age Mapping:**
- "old" = ? (need exact age from paper)
- "Young" = ? (note capital Y)

**Tissue:** Intervertebral disc (spinal disc)
**Species:** Homo sapiens

**Parsing Strategy:**
1. `pd.read_excel(skiprows=1, sheet_name="Raw data")`
2. Remove "T: " prefix from column names
3. Parse complex column structure with regex:
   - Pattern: `LFQ intensity ([^ ]+) (old|Young) ([LAPR]) (.+)`
   - Groups: disc_level, age_group, region, tissue_type
4. Create Sample_ID: `{disc_level}_{age}_{region}_{tissue}`

**Complexity:**
- Most complex column structure (4-level hierarchy)
- Need regex parsing
- Capitalization inconsistency ("old" vs "Young")
- Multiple tissue subtypes

**Gaps:**
- ‚ùå Missing exact age definitions
- ‚ùå Need tissue/region definitions from paper
- ‚ùå Missing LFQ normalization details

**Estimated Rows:** 3,157 proteins √ó 66 samples = ~208,000 rows (!!)
‚ö†Ô∏è **This alone exceeds the 150k target!**

---

### 11. Tsumagari et al. 2023 ‚ö†Ô∏è

**Status:** ‚ö†Ô∏è ANALYZED BUT NOT PARSED

**Files Available:**
- Multiple MOESM files (2-8)
- **Primary:** `41598_2023_45570_MOESM3_ESM.xlsx` (3.19 MB)

**Structure:**
- **Skip rows:** 0
- Sheet: "expression"
- Dimensions: 6,821 rows √ó 32 columns
- Abundance: Columns 0-17 (18 samples)
  - `Cx_3mo_1-6` (6 samples, 3 months)
  - `Cx_15mo_1-6` (6 samples, 15 months)
  - `Cx_24mo_1-6` (6 samples, 24 months)
- Protein IDs: `UniProt accession` (column 30)
- Gene symbols: `Gene name` (column 31)

**Age Mapping:**
- 3mo = 3 months
- 15mo = 15 months
- 24mo = 24 months

**Tissue:** Cx = Cortex (brain)
**Species:** Mus musculus

**Parsing Strategy:**
1. `pd.read_excel(sheet_name="expression")`
2. Extract age from column pattern: `Cx_{age}mo_{replicate}`
3. Protein info in metadata columns at end (30-31)

**Complexity:**
- Protein metadata at end (columns 30-31) instead of beginning
- No Protein_Name column
- 18 samples (6 replicates √ó 3 ages)

**Gaps:**
- ‚ùå Missing abundance unit (raw intensity? LFQ? log2?)
- ‚ùå Need Method confirmation
- ‚ùå Confirm tissue = Cortex and species = Mus musculus

**Estimated Rows:** 6,821 proteins √ó 18 samples = ~123,000 rows

---

## Total Row Count Projection

### Conservative Estimate (Minimum)

| Study | Rows | Notes |
|-------|------|-------|
| Angelidis 2019 | 38,057 | ‚úÖ Confirmed |
| Ariosa-Morejon 2021 | 5,000 | ‚ö†Ô∏è Estimate (4 sheets) |
| Caldeira 2017 | 1,078 | ‚úÖ Confirmed |
| Chmelova 2023 | 19,000 | ‚ö†Ô∏è Estimate (transposed) |
| Dipali 2023 | 8,168 | ‚úÖ Confirmed |
| Li dermis 2021 | 4,000 | ‚ö†Ô∏è Estimate |
| Li pancreas 2021 | 50,000 | ‚ö†Ô∏è Estimate |
| Ouni 2022 | 0 | ‚ùå May exclude |
| Randles 2021 | 31,000 | ‚ö†Ô∏è Estimate |
| Tam 2020 | 208,000 | ‚ö†Ô∏è Estimate (huge!) |
| Tsumagari 2023 | 123,000 | ‚ö†Ô∏è Estimate |
| **TOTAL** | **487,303** | üéØ **Exceeds 150k target!** |

### With Tam 2020 Capped (Realistic)

If Tam 2020 is too large, may need to subsample:
- Option A: Parse all (208k rows from Tam alone)
- Option B: Parse subset of samples (e.g., 20 samples ‚Üí ~63k rows)

**Even without Tam:** 279,303 rows (still exceeds 150k)

**Conclusion:** 150k row target is **easily achievable** even if Ouni and Tam are excluded.

---

## Protein Count Projection

### Unique Proteins per Study

| Study | Unique Proteins | Species |
|-------|----------------|---------|
| Angelidis 2019 | 5,189 | Mouse |
| Ariosa-Morejon 2021 | ~1,000 | ? |
| Caldeira 2017 | 77 | Cow |
| Chmelova 2023 | 3,830 | Mouse |
| Dipali 2023 | 4,084 | Mouse |
| Li dermis 2021 | 264 | Human |
| Li pancreas 2021 | 2,064 | Human |
| Randles 2021 | 2,610 | Human |
| Tam 2020 | 3,157 | Human |
| Tsumagari 2023 | 6,821 | Mouse |
| **TOTAL (no overlap)** | **~29,000** | - |

**Actual unique (with overlap):** ~12,000-15,000 (conservative)
- Mouse proteins overlap across Angelidis, Chmelova, Dipali, Tsumagari
- Human proteins overlap across Li, Randles, Tam

**Conclusion:** 12k protein target is **achievable**.

---

## Critical Implementation Issues

### Issue 1: Missing Paper Citations üî¥

**Impact:** Fails Tier 3 (Traceability) - 4 criteria

**Required for EACH study:**
- PMID or DOI
- Exact age definitions from Methods section (with page/paragraph)
- Exact abundance formula from Methods section (with page/paragraph)

**Current Status:**
- ‚ùå 0/11 studies have complete citations
- ‚ö†Ô∏è Some studies have partial info from existing docs

**Resolution:**
- Option A: Search PubMed for PMIDs (5-10 min)
- Option B: Infer from file structure + supplementary materials
- Option C: Mark as "inferred from data" in Parsing_Notes

**Recommendation:** Option A (search PMIDs) for best score

---

### Issue 2: Normalization Strategy Undefined üî¥

**Impact:** Fails Tier 0.3 and Tier 2.4 criteria

**Problem:** 9 different abundance units across studies:
- log2_intensity (Angelidis)
- iBAQ ratio (Ariosa-Morejon)
- normalized_ratio (Caldeira)
- log2 (Chmelova)
- LFQ (Dipali)
- log2_FOT (Li dermis)
- Unknown (Li pancreas, Randles, Tam, Tsumagari)

**Decision Required:**
- Convert all to common scale? (e.g., log2)
- Preserve original units?
- Provide both?

**Recommendation:** Preserve original units
- Less error-prone
- Maintain fidelity to papers
- Document clearly in Abundance_Unit + Parsing_Notes
- Users can normalize downstream if needed

**Deliverable:** `knowledge_base/03_normalization_strategy.md`

---

### Issue 3: Ouni 2022 Ambiguity üî¥

**Impact:** Affects dataset count (10 vs 11) and row target

**Conflicting Information:**
- PARSER_CONFIG: "SKIP - Literature mining"
- Task V2: "Parse as dataset #8"

**Investigation Plan:**
1. Open `Supp Table 1.xlsx`
2. Check for protein abundance columns
3. Look for sample/age structure
4. Verify if proteomics or literature data

**Decision Tree:**
- If proteomics ‚Üí Parse (target: 11 datasets, ‚â•150k rows)
- If literature ‚Üí Exclude (target: 10 datasets, ‚â•140k rows)

**Time Required:** 5 min to investigate

---

### Issue 4: Parsing_Notes Generation üî¥

**Impact:** Fails Tier 5.3 criterion (required for PASS)

**Requirement:** Every row must have Parsing_Notes with:
1. Age source and reasoning
2. Abundance source and location
3. Any transformations applied

**Example:**
```
"Age=24mo from col name 'old_1' per Methods p.2; Abundance from MOESM5 sheet 'Proteome' col AF; log2 already applied per Methods p.3"
```

**Implementation Options:**

**Option A: Hardcode per study**
```python
PARSING_NOTES = {
    'Angelidis_2019': {
        'old_1': "Age=24mo from col 'old_1' per Methods p.2; Abundance from MOESM5 sheet 'Proteome' col AF; log2 applied per Methods p.3",
        # ... for each sample
    }
}
```
‚ùå Repetitive, error-prone

**Option B: Template-based**
```python
CONFIGS = {
    'Angelidis_2019': {
        'age_template': 'Age={age}{unit} from col name {col_name} per Methods p.2',
        'abundance_template': 'Abundance from {file} sheet {sheet} col {col}; {transform} per Methods p.3',
        'age_mapping': {'old': (24, 'mo'), 'young': (3, 'mo')},
    }
}

# Generate per row:
notes = f"{age_template.format(...)}; {abundance_template.format(...)}"
```
‚úÖ Scalable, maintainable

**Recommendation:** Option B (Template-based)

---

### Issue 5: Complex Format Parsing üî¥

**High-Risk Datasets:**

**Chmelova 2023 (Transposed):**
- Risk: Transpose error, wrong axis
- Mitigation: Validate dimensions before/after transpose
- Test: Check that genes become row index

**Tam 2020 (Hierarchical):**
- Risk: Regex parsing failure, incorrect column mapping
- Mitigation: Test regex on sample columns first, validate all 66 columns parsed
- Test: Verify all disc_level √ó age √ó region √ó tissue combinations

**Parsing Order:** Do these LAST (after validating simpler datasets)

---

## Tier 0: Knowledge Base Checklist

### 0.1 Dataset Inventory ‚ùå

**Status:** INCOMPLETE

**Required:** `knowledge_base/00_dataset_inventory.md`

**Content:**
| Study | File | Format | Sheet | Est. Rows | Notes |
|-------|------|--------|-------|-----------|-------|
| Angelidis 2019 | MOESM5_ESM.xlsx | Excel | Proteome | 38,057 | Main data |
| ... | ... | ... | ... | ... | ... |

**Gaps:**
- ‚ùå Missing exact file paths for all 11 studies
- ‚ùå Missing sheet names where applicable
- ‚ùå Missing row count verification for 7 studies
- ‚ùå Ouni 2022 not investigated

**Action:** Create inventory (10 min)

---

### 0.2 Paper Analysis ‚ùå

**Status:** INCOMPLETE

**Required:** 11 files in `knowledge_base/01_paper_analysis/`

**Template per paper:**
```markdown
# [Study] et al. [Year] - Analysis

## 1. Paper Overview
- Title: [exact title]
- PMID: [ID]
- Tissue: [organ]
- Species: [organism]
- Age groups: [definitions from paper]

## 2. Data Files Available
- File: [name]
- Sheet: [sheet name]
- Rows: [count]
- Columns: [list]

## 3. Column Mapping to Schema
| Schema Column | Source Location | Paper Section | Reasoning |
|---------------|----------------|---------------|-----------|
| Protein_ID | "Majority protein IDs" | Methods p.3 | UniProt IDs, semicolon-separated, take first |

## 4. Abundance Calculation
- Formula from paper: [exact quote]
- Unit: [log2_intensity / LFQ / etc]
- Already normalized: [YES/NO]

## 5. Ambiguities & Decisions
- Ambiguity #1: [describe]
  - Decision: [what decided]
  - Reasoning: [why]
```

**Current Status:**
- ‚ùå 0/11 papers analyzed with this template
- ‚ö†Ô∏è Some info exists in dataset_analysis_summary.md but incomplete

**Action:** Create 11 paper analysis files (30-40 min)

---

### 0.3 Normalization Strategy ‚ùå

**Status:** NOT CREATED

**Required:** `knowledge_base/03_normalization_strategy.md`

**Content:**
1. List of all abundance units across studies
2. Decision on transformation approach
3. Reasoning for decision
4. Implementation notes

**Recommendation:**
```markdown
# Normalization Strategy

## Decision: PRESERVE ORIGINAL UNITS

### Rationale:
1. Maintains fidelity to source papers
2. Avoids transformation errors
3. Different units may not be directly comparable (iBAQ vs LFQ vs spectral counts)
4. Downstream users can normalize if needed

### Implementation:
- Store original values in Abundance column
- Document unit in Abundance_Unit column
- Reference transformation status in Parsing_Notes
- If already normalized in paper (e.g., log2), preserve and document

### Units by Study:
- Angelidis 2019: log2_intensity (already log2-transformed)
- Ariosa-Morejon 2021: iBAQ (raw) or ratio (H/L)
- ...
```

**Action:** Create strategy doc (5 min)

---

### 0.4 Implementation Plan ‚ùå

**Status:** NOT CREATED

**Required:** `knowledge_base/04_implementation_plan.md`

**Content:**
1. Parsing order (easiest to hardest)
2. Per-study parsing logic
3. Common functions to implement
4. Validation checkpoints

**Recommendation:**
```markdown
# Implementation Plan

## Phase 1: Easy Datasets (3 studies)
1. Caldeira 2017 - Simple 3-group, already working
2. Dipali 2023 - Simple 2-group, already working
3. Tsumagari 2023 - Standard format

## Phase 2: Moderate Datasets (5 studies)
4. Angelidis 2019 - Already working
5. Ariosa-Morejon 2021 - Multi-sheet
6. Li dermis 2021 - Skip rows
7. Li pancreas 2021 - Age prefix codes
8. Randles 2021 - Dual tissue

## Phase 3: Complex Datasets (2 studies)
9. Tam 2020 - Hierarchical columns
10. Chmelova 2023 - Transposed matrix

## Phase 4: Unknown (1 study)
11. Ouni 2022 - Investigate first

## Validation Checkpoints:
- After Study 3: Verify 3 easy datasets
- After Study 8: Verify all moderate datasets
- After Study 10: Verify complex datasets
- After Study 11: Final validation
```

**Action:** Create implementation plan (5 min)

---

## Tier 1-5: Feasibility Assessment

### ‚úÖ TIER 1: DATA COMPLETENESS (Achievable)

- ‚úÖ 1.1: Parse ALL 11 datasets - **FEASIBLE** (10 confirmed + Ouni TBD)
- ‚úÖ 1.2: ‚â•150,000 rows - **EASILY ACHIEVABLE** (projected 280k-480k)
- ‚úÖ 1.3: ‚â•12,000 proteins - **ACHIEVABLE** (projected 12k-15k)
- ‚úÖ 1.4: No mock data - **GUARANTEED** (all from real files)

**Risk:** LOW

---

### ‚ö†Ô∏è TIER 2: DATA QUALITY (Mostly Achievable)

- ‚úÖ 2.1: Zero critical nulls - **ACHIEVABLE** (validate during parsing)
- ‚úÖ 2.2: Protein IDs standardized - **ACHIEVABLE** (UniProt or Gene symbols)
- ‚ö†Ô∏è 2.3: Age groups correctly identified - **REQUIRES PAPER ACCESS**
- ‚ö†Ô∏è 2.4: Abundance units with citations - **REQUIRES PAPER ACCESS**
- ‚úÖ 2.5: Tissue metadata complete - **ACHIEVABLE** (from papers/files)
- ‚úÖ 2.6: Sample IDs preserve structure - **ACHIEVABLE** (from column names)

**Risk:** MEDIUM (depends on paper access)

---

### üî¥ TIER 3: COLUMN TRACEABILITY (High Risk)

- ‚ö†Ô∏è 3.1: Column mapping documentation - **REQUIRES PAPER ACCESS**
- üî¥ 3.2: Paper PDF references (PMID/DOI) - **CAN SEARCH PUBMED**
- üî¥ 3.3: Abundance formulas documented - **REQUIRES PAPER ACCESS**
- ‚ö†Ô∏è 3.4: Ambiguities documented - **ACHIEVABLE** (can document unknowns)

**Risk:** HIGH (paper access critical)

**Mitigation:**
- Search PubMed for PMIDs (easy)
- Use supplementary materials for formulas
- Document "inferred from data structure" where needed

---

### ‚úÖ TIER 4: PROGRESS TRACKING (Achievable)

- ‚úÖ 4.1: Task decomposition - **DONE** (this analysis plan)
- ‚úÖ 4.2: Real-time progress logging - **ACHIEVABLE** (update log after each study)
- ‚úÖ 4.3: Validation checkpoints - **ACHIEVABLE** (built into phases)

**Risk:** LOW

---

### ‚úÖ TIER 5: DELIVERABLES (Achievable)

- ‚úÖ 5.1: Complete file set - **ACHIEVABLE**
- ‚úÖ 5.2: Validation report - **ACHIEVABLE** (auto-generate)
- ‚ö†Ô∏è 5.3: Parsing_Notes in every row - **REQUIRES IMPLEMENTATION**
- ‚úÖ 5.4: Reproducible code - **EXTEND V1 CODE**

**Risk:** LOW (except Parsing_Notes needs new code)

---

## Overall Feasibility: ‚ö†Ô∏è ACHIEVABLE WITH CAVEATS

### Can PASS (25/25 criteria): ‚ö†Ô∏è MAYBE

**Blockers:**
1. üî¥ **Paper access** - Need Methods sections for exact formulas and age definitions
2. üî¥ **Parsing_Notes implementation** - Requires code extension
3. ‚ö†Ô∏è **Ouni 2022 resolution** - Need to investigate if parseable
4. ‚ö†Ô∏è **Complex parsing** - Chmelova transpose and Tam hierarchical columns

**If papers accessible:** ‚úÖ HIGH CONFIDENCE (90% pass probability)
**Without paper access:** ‚ö†Ô∏è MEDIUM CONFIDENCE (60% pass probability - can document "inferred")

---

## Recommended Execution Plan

### Step 1: Resolve Blockers (20 min)

**1.1 Search PMIDs (10 min)**
- PubMed search for all 11 studies
- Record PMID/DOI in inventory

**1.2 Investigate Ouni 2022 (5 min)**
- Open Supp Table files
- Determine if proteomics
- Decide include/exclude

**1.3 Create Knowledge Base Structure (5 min)**
```bash
mkdir -p knowledge_base/01_paper_analysis
touch knowledge_base/00_dataset_inventory.md
touch knowledge_base/02_column_mapping_strategy.md
touch knowledge_base/03_normalization_strategy.md
touch knowledge_base/04_implementation_plan.md
```

---

### Step 2: Complete Tier 0 (40 min)

**2.1 Dataset Inventory (10 min)**
- Fill `00_dataset_inventory.md` table
- Verify all file paths
- Confirm row counts where known

**2.2 Paper Analysis (25 min)**
- Create 11 paper analysis files
- Fill sections 1-2 (overview, files) from existing docs
- Fill section 3 (column mapping) from DATASET_STRUCTURES
- Leave sections 4-5 (formulas, ambiguities) as "TBD - need paper"
- OR search supplementary materials for this info

**2.3 Normalization Strategy (5 min)**
- Create `03_normalization_strategy.md`
- Document "preserve original units" decision
- List all units by study

**2.4 Implementation Plan (already done in this analysis!)**
- Copy Phase 1-4 from this document
- Add checkpoint validation logic

---

### Step 3: Extend V1 Code (30 min)

**3.1 Add Parsing_Notes Generation (20 min)**
- Update STUDY_CONFIGS with templates
- Add `generate_parsing_notes()` function
- Update parsers to include notes column

**3.2 Add New Study Configs (10 min)**
- Ariosa-Morejon 2021 (multi-sheet)
- Li dermis 2021 (skip rows)
- Li pancreas 2021 (age prefixes)
- Randles 2021 (dual tissue)
- Tam 2020 (hierarchical, complex)
- Chmelova 2023 (transpose)
- Tsumagari 2023 (standard)
- Ouni 2022 (if applicable)

---

### Step 4: Parse Datasets (90-120 min)

**4.1 Phase 1: Easy (20 min)**
- Caldeira 2017 ‚úÖ (reuse V1)
- Dipali 2023 ‚úÖ (reuse V1)
- Tsumagari 2023 üÜï (standard)
- Checkpoint validation

**4.2 Phase 2: Moderate (40-50 min)**
- Angelidis 2019 ‚úÖ (reuse V1)
- Ariosa-Morejon 2021 üÜï (multi-sheet)
- Li dermis 2021 üÜï (skip rows)
- Li pancreas 2021 üÜï (age prefixes)
- Randles 2021 üÜï (dual tissue)
- Checkpoint validation

**4.3 Phase 3: Complex (30-40 min)**
- Tam 2020 üÜï (hierarchical)
- Chmelova 2023 üÜï (transpose)
- Checkpoint validation

**4.4 Phase 4: Unknown (10 min)**
- Ouni 2022 (if applicable)
- Final validation

---

### Step 5: Validation & Documentation (30 min)

**5.1 Generate Deliverables (15 min)**
- Combine all CSVs ‚Üí `ecm_atlas_unified.csv`
- Generate `metadata.json`
- Create `validation_report.md`

**5.2 Validation Checks (15 min)**
- Verify 25/25 criteria
- Spot-check 20 random rows
- Verify Parsing_Notes completeness
- Check null counts
- Verify protein counts

---

## Total Time Estimate

| Phase | Time | Status |
|-------|------|--------|
| Step 1: Resolve Blockers | 20 min | ‚è≥ Required |
| Step 2: Complete Tier 0 | 40 min | ‚è≥ Required |
| Step 3: Extend V1 Code | 30 min | ‚è≥ Required |
| Step 4: Parse Datasets | 90-120 min | ‚è≥ Required |
| Step 5: Validation | 30 min | ‚è≥ Required |
| **TOTAL** | **3h 30m - 4h 00m** | - |

**Matches task estimate:** 3h 15m - 4h 30m ‚úÖ

---

## Risk Mitigation Strategies

### Risk 1: Paper Access Failure

**If cannot access papers for formulas/age definitions:**

**Mitigation:**
1. Use supplementary materials (often have Methods)
2. Infer from data structure (e.g., column names)
3. Document as "inferred from data structure" in Parsing_Notes
4. Mark confidence level: High / Medium / Low

**Example Parsing_Notes (without paper):**
```
"Age=24mo inferred from col name 'old_1' (likely 24 months based on study context); Abundance from MOESM5 sheet 'Proteome' col AF; appears log2-transformed based on value range"
```

**Impact:** May lose points on Tier 3.3 (exact formula quotes) but still pass other criteria

---

### Risk 2: Ouni 2022 Not Proteomics

**If Ouni is literature mining data:**

**Mitigation:**
1. Exclude from parsing (10 datasets instead of 11)
2. Adjust row target: ‚â•140,000 (still achievable with 280k-480k projected)
3. Document exclusion reason in validation report
4. Still meets 1.1 criterion (parse ALL parseable datasets)

**Impact:** Minimal - still exceeds row/protein targets

---

### Risk 3: Complex Parsing Errors

**If Chmelova transpose or Tam hierarchical parsing fails:**

**Mitigation:**
1. Debug incrementally (test on 10 rows first)
2. Validate dimensions/structure before processing
3. If unfixable, document as "excluded due to format complexity"
4. Still have 9 datasets ‚Üí ~150k-200k rows (meets target)

**Impact:** Could lose 1-2 datasets but still pass completeness criteria

---

### Risk 4: Parsing_Notes Implementation Incomplete

**If cannot generate complete Parsing_Notes for all rows:**

**Mitigation:**
1. Implement basic version: "Age from {source}; Abundance from {source}"
2. Add paper citations where available
3. Mark incomplete notes as "reasoning inferred from data structure"

**Impact:** May lose points on 5.3 but could still pass if other criteria met

---

## Success Probability Assessment

### Scenario 1: Ideal Conditions ‚úÖ
**Conditions:** Papers accessible, all datasets parseable, code works
- **Probability:** 90%
- **Expected Score:** 25/25 (PASS)

### Scenario 2: Paper Access Limited ‚ö†Ô∏è
**Conditions:** Some papers accessible, Ouni excluded, code works
- **Probability:** 70%
- **Expected Score:** 22-24/25 (borderline PASS/FAIL)

### Scenario 3: Multiple Issues üî¥
**Conditions:** No papers, Ouni excluded, 1-2 complex datasets fail
- **Probability:** 40%
- **Expected Score:** 18-21/25 (FAIL)

### Recommended Strategy: Hybrid Approach ‚úÖ

**Goal:** Maximize success probability

**Approach:**
1. ‚úÖ Complete Tier 0 fully (4/4 criteria) - use available info
2. ‚úÖ Parse all confirmed datasets (prioritize easy/moderate)
3. ‚úÖ Document limitations clearly ("inferred", "assumed", "estimated")
4. ‚úÖ Implement basic Parsing_Notes (can enhance later)
5. ‚úÖ Validate incrementally (catch issues early)

**Expected Outcome:** 22-25/25 criteria (PASS or borderline)

---

## Key Takeaways for Execution

### ‚úÖ DO THIS:

1. **Complete Tier 0 FIRST** - No coding until knowledge base done
2. **Search PMIDs immediately** - Easy wins for Tier 3.2
3. **Investigate Ouni now** - Resolve ambiguity early
4. **Parse incrementally** - Easy ‚Üí Moderate ‚Üí Complex
5. **Validate at checkpoints** - Catch issues before final
6. **Document limitations** - "Inferred" is better than wrong
7. **Reuse V1 code** - Proven architecture, just extend

### ‚ùå DON'T DO THIS:

1. **Don't code before Tier 0** - Will fail planning criteria
2. **Don't assume paper content** - Verify or mark as inferred
3. **Don't parse complex first** - Risk wasting time on hardest
4. **Don't skip checkpoints** - Errors compound
5. **Don't mock data** - Instant disqualification
6. **Don't leave Parsing_Notes empty** - Required for pass

---

## Final Recommendation

### PROCEED WITH EXECUTION: ‚úÖ YES

**Rationale:**
1. Task is achievable within 3-4 hours
2. Row/protein targets are easily met (even with exclusions)
3. V1 code provides solid foundation
4. Most criteria are implementation-focused (doable)
5. Paper access limitations can be mitigated with "inferred" documentation

**Predicted Outcome:** 22-25/25 criteria (PASS likely)

**Critical Success Factors:**
1. Complete Tier 0 (4 criteria) - 60 min
2. Implement Parsing_Notes - 20 min
3. Parse 9-11 datasets - 90-120 min
4. Validate thoroughly - 30 min

**Next Action:** Execute Step 1 (Resolve Blockers) - 20 min

---

## Appendix A: PMID Search Results

**Note:** To be filled during execution

| Study | PMID | DOI | Title |
|-------|------|-----|-------|
| Angelidis 2019 | ? | ? | ? |
| Ariosa-Morejon 2021 | ? | ? | ? |
| Caldeira 2017 | ? | ? | ? |
| Chmelova 2023 | ? | ? | ? |
| Dipali 2023 | ? | ? | ? |
| Li 2021 (both) | ? | ? | ? |
| Ouni 2022 | ? | ? | ? |
| Randles 2021 | ? | ? | ? |
| Tam 2020 | ? | ? | ? |
| Tsumagari 2023 | ? | ? | ? |

---

## Appendix B: Ouni 2022 Investigation

**Note:** To be filled during execution

**Files checked:** Supp Table 1-4.xlsx

**Structure:**
- [ ] Has protein IDs? (Y/N)
- [ ] Has abundance columns? (Y/N)
- [ ] Has age/sample structure? (Y/N)

**Decision:**
- [ ] PARSE (proteomics data)
- [ ] EXCLUDE (literature mining)

**Reasoning:** [To be filled]

---

## Appendix C: Code Extension Checklist

### Parsing_Notes Implementation

- [ ] Add templates to STUDY_CONFIGS
- [ ] Create `generate_parsing_notes()` function
- [ ] Update `parse_wide_format()` to include notes
- [ ] Update `parse_comparative_format()` to include notes
- [ ] Test notes generation on Angelidis dataset
- [ ] Verify notes format matches requirements

### New Study Parsers

- [ ] Ariosa-Morejon 2021 (multi-sheet handler)
- [ ] Li dermis 2021 (skiprows=3 handler)
- [ ] Li pancreas 2021 (age prefix parser)
- [ ] Randles 2021 (dual tissue handler)
- [ ] Tam 2020 (regex column parser)
- [ ] Chmelova 2023 (transpose handler)
- [ ] Tsumagari 2023 (metadata-at-end handler)

---

**END OF ANALYSIS RESULTS**

**Status:** ‚úÖ ANALYSIS COMPLETE - READY FOR EXECUTION
**Next Step:** Begin Step 1 (Resolve Blockers)
**Estimated Time to Completion:** 3h 30m - 4h 00m
